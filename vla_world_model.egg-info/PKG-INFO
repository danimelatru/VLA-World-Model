Metadata-Version: 2.4
Name: vla_world_model
Version: 0.1.0
Summary: Modular VLA: Interpretable World Models for Safe Robotic Control
Author-email: Daniel Fern√°ndez de la Mela <daniel.fernandezdelamela@student-cs.fr>
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: numpy
Requires-Dist: torch
Requires-Dist: torchvision
Requires-Dist: h5py
Requires-Dist: pyyaml
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: isort; extra == "dev"
Requires-Dist: flake8; extra == "dev"

# üß† Modular VLA: Interpretable World Models for Safe Robotic Control

> **A "System 2" approach to Embodied AI:** Decoupling reasoning (World Models) from acting (Policy) to enable interpretability and safer control.

![Banner](assets/physics_check_1.png)
*(Fig 1: The model predicting future robot states with high precision based on physics understanding)*

## üöÄ The Concept
Standard Vision-Language-Action (VLA) models like RT-2 or OpenVLA are "black boxes": they map pixels directly to actions. If they fail, we don't know why.

**This project proposes a Modular Architecture:**
1.  **Frozen Perception:** Uses state-of-the-art encoders (SigLIP/CLIP) to process visual data.
2.  **Trainable World Model:** An explicit neural physics engine that learns $P(s_{t+1} | s_t, a_t)$. It allows the robot to "imagine" the outcome of an action before executing it.
3.  **Policy Network:** A control module that acts based on the World Model's understanding.

**Why this matters for Industry?**
* **Safety:** We can audit the robot's "imagination" to detect hallucinations before they cause accidents.
* **Data Efficiency:** World Models learn physics from offline data, reducing the need for dangerous real-world interaction.

## üõ†Ô∏è Architecture

The system is composed of two learnable components trained on the **Robomimic (Lift-PH)** dataset:

### 1. The World Model (The Brain)
* **Input:** Current State Embedding + Action + Language Instruction.
* **Output:** Predicted Next State Embedding.
* **Architecture:** Residual MLP with LayerNorm and GELU activations.
* **Performance:** Achieved **MSE < 1e-4** on one-step lookahead dynamics.

### 2. The Policy (The Body)
* **Input:** State Embedding + Language Instruction.
* **Output:** 7-DoF Robot Action (End-effector pose + Gripper).
* **Training:** Behavior Cloning (Supervised Learning) from expert demonstrations.

## üìÇ Project Structure

```text
vla_world_model/
‚îú‚îÄ‚îÄ data/               # Datasets (HDF5)
‚îú‚îÄ‚îÄ logs/               # Slurm and Training logs
‚îú‚îÄ‚îÄ results/            # Checkpoints and Evaluation Figures
‚îú‚îÄ‚îÄ scripts/            # Training and Evaluation scripts
‚îÇ   ‚îú‚îÄ‚îÄ download_data_hf.py
‚îÇ   ‚îú‚îÄ‚îÄ precompute_embeddings.py
‚îÇ   ‚îú‚îÄ‚îÄ train_world_model.py
‚îÇ   ‚îú‚îÄ‚îÄ train_policy.py
‚îÇ   ‚îú‚îÄ‚îÄ eval_dreaming.py
‚îÇ   ‚îú‚îÄ‚îÄ run_train.slurm
‚îÇ   ‚îú‚îÄ‚îÄ run_policy.slurm
‚îÇ   ‚îî‚îÄ‚îÄ run_eval.slurm
‚îú‚îÄ‚îÄ src/                # Source Code
‚îÇ   ‚îú‚îÄ‚îÄ datasets/       # Data Loaders
‚îÇ   ‚îî‚îÄ‚îÄ models/         # Neural Network Architectures (PyTorch)
‚îú‚îÄ‚îÄ environment.yml     # Conda environment definition
‚îî‚îÄ‚îÄ README.md
```

## ‚ö° Quick Start

### 1. Environment Setup
```bash
conda create -n vla-wm python=3.10
conda activate vla-wm
pip install torch torchvision robomimic h5py huggingface_hub matplotlib
```

### 2. Data Preparation
We use the official **Robomimic Low-Dim** dataset (Lift task) for rapid prototyping.

```bash
# 1. Download dataset from Hugging Face
python scripts/download_data_hf.py

# 2. Process raw data into embeddings (ready for training)
python scripts/precompute_embeddings.py
```

### 3. Training
The system requires training two components sequentially: first the World Model (Physics), then the Policy (Control).

```bash
# Option A: Submit to Slurm (HPC Cluster)
sbatch scripts/run_train.slurm   # Trains World Model
sbatch scripts/run_policy.slurm  # Trains Policy

# Option B: Run locally
python scripts/train_world_model.py
python scripts/train_policy.py
```

### 4. Evaluation ("Dreaming")
Test the model's ability to simulate the future. This script runs a **One-Step Lookahead** validation to verify physics understanding.

```bash
sbatch scripts/run_eval.slurm
# Output figures will be saved to: results/figures/
```

## üìä Results & Analysis

### 1. Physics Understanding (One-Step Lookahead)
The World Model demonstrates a strong grasp of the environment's dynamics. As seen in the validation plots below, the **predicted state (green)** closely tracks the **ground truth (black)**, even capturing sudden changes in direction and velocity.

![Physics Validation](assets/physics_check_1.png)
*(Fig 1: One-step lookahead prediction. The model accurately predicts sharp state transitions, achieving an MSE < 1e-4).*

### 2. Long-Horizon Stability
While the model excels at local dynamics, open-loop "dreaming" over long horizons (50+ steps) exhibits drift (separation between prediction and reality). This is a known phenomenon in simple autoregressive MLP models without latent overshooting or state correction.

| Metric | Value | Interpretation |
| :--- | :--- | :--- |
| **Dynamics Loss (MSE)** | `9e-6` | High precision in immediate physics prediction. |
| **Policy Loss (MSE)** | `0.023` | Effective imitation of expert actions. |

## üîÆ Future Work & Scaling

This project serves as a foundational MVP. To scale this into a production-grade system, the following steps are proposed:

1.  **Vision-Based Inputs:** Replace Low-Dim states with **SigLIP embeddings** extracted from raw pixels (the pipeline is already scaffolded in `precompute_embeddings.py`).
2.  **Transformer Backbone:** Replace the MLP World Model with a **Transformer (GPT-style)** or a **Diffusion Model** to handle longer context windows and reduce long-term drift.
3.  **Closed-Loop Simulation:** Integrate with **MuJoCo** to measure actual task Success Rates (Lift Success %) rather than just trajectory error.

---

## üë®‚Äçüíª Author

**Daniel Fern√°ndez de la Mela**
MSc in AI Student @ CentraleSup√©lec
[LinkedIn](https://www.linkedin.com/in/danielfernandezdelamelatrujillano) | [GitHub](https://github.com/danimelatru)
