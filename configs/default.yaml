training:
  batch_size: 128 # 128 with Gradient Checkpointing should be safe on 32GB VRAM
  epochs: 20
  lr: 0.001
  use_scheduler: true
  warmup_steps: 1000
  save_dir: "./results/checkpoints"
  # device: "cuda"

data:
  dataset_path: "./data/multi_task_embeddings.hdf5"
  num_workers: 4
  text_instruction: "Lift the red cube" # Default for inference, training uses dataset

model:
  text_dim: 512
  hidden_dim: 512

vla:
  model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
  load_in_4bit: false
  use_lora: true
  lora_rank: 32
